# -*- coding: utf-8 -*-
"""Exercise 5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0md9SMZuS5SXKQPWzJ6Bza7KHVWcOsT

*   Why can't we just load vgg16 from torchvision? It would be much easier. The knowledge from Exercise 4 is not used in Exercise 5
*   What should be the end layer of the encoder? Why does it say "The fully connected layers and the last pooling layer are omitted."
*   Should we train the parameters of the encoder or not?
*   What is a good average loss on the validation set?
*   About how many epochs should one train? 
*   Is it normal that after one epoch the model is still very bad? For me it always outputs the tensor for every input.

Ideas:
* Use AdamW, maybe set weight decay manually
* Use data augmentation

# Imports
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import imageio
import pandas as pd

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import torchvision

import os
from contextlib import contextmanager
from IPython.display import clear_output
from time import time
import random

"""# Constants"""

PRODUCTION_MODE = False

SEED = 0
TRAINING_BATCH_SIZE = 7
TRAINING_EPOCHS = 5

LEARNING_RATE = 0.0001
WEIGHT_DECAY = 0
LOSS_WEIGHT_ALPHA = 1.1 # Only used by WeightedMSELoss
USE_DATA_AUGMENTATION = True
AMOUNT_TRAINING_DATA = 10 # Disregarding data augmentation
AMOUNT_VALIDATION_DATA = 10 # Disregarding data augmentation

LOSS_BCE = "LOSS_BCE"
LOSS_WEIGHTED_MSE = "LOSS_WEIGHTED_MSE"
TRAINING_LOSS_CHOICE = LOSS_BCE

TRAINING_STATUS_DISPLAY_INTERVAL = 5 # in seconds

CHECKPOINT_FILE_FOLDER = 'checkpoints/'
CHECKPOINT_FILE_PREFIX = 'checkpoint_'
CHECKPOINT_FILE_POSTFIX = '.pth'
TRAINING_DATA_FOLDER = 'training_data/'
VALIDATION_DATA_FOLDER = 'validation_data/'

MOODLE_SESSION = 'gu58lu5r7s5k3113i4d37chj56'

os.makedirs(CHECKPOINT_FILE_FOLDER, exist_ok=True)

"""# Defining device"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print('PyTorch device:', device)

"""# Seeding"""

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED);

"""# Downloading files"""

def download_from_moodle(url, local_filename, unzip=False, unzip_to='.'):
  import os
  import requests
  import shutil
  import zipfile

  if os.path.isfile(local_filename):
    print("Skipping:", local_filename, "already exists.")
    return

  print("Downloading:", local_filename)
  with requests.get(url, stream=True, cookies={'MoodleSession': MOODLE_SESSION}) as r:
      with open(local_filename, 'wb') as f:
          shutil.copyfileobj(r.raw, f)

  if unzip:
    print("Unzipping to:", unzip_to)
    with zipfile.ZipFile(local_filename, 'r') as zip_ref:
      zip_ref.extractall(unzip_to)
  
  print("Done!")

download_from_moodle('https://lernen.min.uni-hamburg.de/mod/resource/view.php?id=6404', 'project_training_data.zip', unzip=True, unzip_to=TRAINING_DATA_FOLDER)

download_from_moodle('https://lernen.min.uni-hamburg.de/mod/resource/view.php?id=6405', 'project_validation_data.zip', unzip=True, unzip_to=VALIDATION_DATA_FOLDER)

"""# Data preprocessing

## Extracting paths
"""

def read_txt_lines(filename, prepend):
		lines = []
		with open(filename, 'r') as file:
				for line in file: 
						line = line.strip()
						lines.append(prepend + line)
		return lines

image_paths_unshuffled = read_txt_lines(TRAINING_DATA_FOLDER + 'train_images.txt', TRAINING_DATA_FOLDER) + \
                  read_txt_lines(VALIDATION_DATA_FOLDER + 'val_images.txt', VALIDATION_DATA_FOLDER)

fixation_paths_unshuffled = read_txt_lines(TRAINING_DATA_FOLDER + 'train_fixations.txt', TRAINING_DATA_FOLDER) + \
                  read_txt_lines(VALIDATION_DATA_FOLDER + 'val_fixations.txt', VALIDATION_DATA_FOLDER)

image_paths_unshuffled[0], image_paths_unshuffled[33], image_paths_unshuffled[-1], \
fixation_paths_unshuffled[0], fixation_paths_unshuffled[33], fixation_paths_unshuffled[-1]

z = list(zip(image_paths_unshuffled, fixation_paths_unshuffled))
random.shuffle(z)
image_paths, fixation_paths = zip(*z)

image_paths[0], image_paths[33], image_paths[-1], \
fixation_paths[0], fixation_paths[33], fixation_paths[-1]

len(image_paths), len(fixation_paths)

image_paths_training, fixation_paths_training = image_paths[:AMOUNT_TRAINING_DATA], fixation_paths[:AMOUNT_TRAINING_DATA]
image_paths_validation, fixation_paths_validation = image_paths[AMOUNT_TRAINING_DATA:AMOUNT_TRAINING_DATA+AMOUNT_VALIDATION_DATA], fixation_paths[AMOUNT_TRAINING_DATA:AMOUNT_TRAINING_DATA+AMOUNT_VALIDATION_DATA]

len(image_paths_training), len(fixation_paths_training), len(image_paths_validation), len(fixation_paths_validation)

"""## Defining dataset class"""

class FixationDataset(Dataset):
	def __init__(self, image_filenames, fixation_filenames, transform=None, add_flip=False):
		self.image_filenames = image_filenames
		self.fixation_filenames = fixation_filenames
		self.transform = transform
		self.add_flip = add_flip

		assert(len(self.image_filenames) == len(self.fixation_filenames))
	
		self.images = []
		self.fixations = []
	
		for index in range(len(self.image_filenames)):
			if index % 100 == 0:
				print('Loading', index + 1, 'out of', len(self.image_filenames))
			self.load_image_and_fixation(index) 
	
		assert(len(self.images) == len(self.fixations))

	def load_image_and_fixation(self, index):
		image_path = self.image_filenames[index]
		image = imageio.imread(image_path)

		fixation_path = self.fixation_filenames[index]
		fixation = imageio.imread(fixation_path)
	
		self.save_sample({'image': image, 'fixation': fixation})
	
		if self.add_flip:
			self.save_sample({'image': np.fliplr(image), 'fixation': np.fliplr(fixation)})
	
	def save_sample(self, sample):
		if self.transform:
			sample = self.transform(sample)

		self.images.append(sample['image'])
		self.fixations.append(sample['fixation'])

	def __len__(self):
		return len(self.images)

	def __getitem__(self, index):
		return {'image': self.images[index], 'fixation': self.fixations[index]}

"""## Defining transforms"""

class FixationTransform():
  def __call__(self, sample):
    image, fixation = sample['image'], sample['fixation']
    return { 'image': self.after(self.image(self.before(image))), 'fixation': self.after(self.fixation(self.before(fixation))) }
  
  def before(self, data): return data
  def image(self, data): return data
  def fixation(self, data): return data
  def after(self, data): return data

class RescalingTransform(FixationTransform):
  def before(self, data):
    return data.astype(np.float32)/255.0

class TensorTransform(FixationTransform):
  def image(self, data):
    return data.transpose((2,0,1))
  
  def fixation(self, data):
    return np.expand_dims(data, axis=0)

  def after(self, data):
    return torch.from_numpy(data)

class NormalizationTransform(FixationTransform):
  def __init__(self):
    self.normalizer = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])
  
  def image(self, data):
    return self.normalizer(data)

class EncoderTransform(FixationTransform):
  def __init__(self):
    self.vgg16 = torchvision.models.vgg16(pretrained=True).features
    self.vgg16[30] = nn.Identity()
    for p in self.vgg16.parameters(): p.requires_grad = False
    
    self.vgg16.to(device)

  def image(self, data):
    with torch.no_grad():
      return self.vgg16(data.unsqueeze(0).to(device)).squeeze(0)

final_transform = torchvision.transforms.Compose([RescalingTransform(), TensorTransform(), NormalizationTransform(), EncoderTransform()])

"""## Creating datasets"""

ds_train = FixationDataset(
    image_filenames=image_paths_training,
    fixation_filenames=fixation_paths_training,
    add_flip=USE_DATA_AUGMENTATION,
    transform=final_transform
)

ds_train_notransform = FixationDataset(
    image_filenames=image_paths_training,
    fixation_filenames=fixation_paths_training,
    add_flip=USE_DATA_AUGMENTATION
)

ds_validation = FixationDataset(
    image_filenames=image_paths_validation,
    fixation_filenames=fixation_paths_validation,
    transform=final_transform
)

ds_validation_notransform = FixationDataset(
    image_filenames=image_paths_validation,
    fixation_filenames=fixation_paths_validation,
)

len(ds_train), len(ds_validation), len(ds_train_notransform), len(ds_validation_notransform)

ds_train[0]['image'].shape, ds_train[0]['fixation'].shape

dl_train = DataLoader(ds_train, batch_size=TRAINING_BATCH_SIZE, shuffle=True)

"""# Decoder build"""

class Decoder(nn.Module):
  def __init__(self):
    super().__init__()

    self.sequential = nn.Sequential(
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),

        nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),

        nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),

        nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),

        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
        nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, padding=0),
    )
  
    # self.sequential = nn.Sequential(
    #     nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
    #
    #     nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
    #
    #     nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
    #
    #     nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
    #
    #     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.Dropout2d(),
    #
    #     nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, padding=0),
    # )

  def forward(self, xb):
    return self.sequential(xb)

net = Decoder()
net.to(device);

"""# Loss build"""

class WeightedMSELoss():
  def __call__(self, output_fixation, true_fixation):
    mse = nn.functional.mse_loss(output_fixation, true_fixation, reduction='none')
    weights = 1/(LOSS_WEIGHT_ALPHA - true_fixation)**2
    weighted_mse = weights * mse

    return weighted_mse.mean()

class BCELossWithDownsampling():
  def __init__(self):
    self.downsample = nn.AvgPool2d(4, stride=4, count_include_pad=False)
    self.loss_function = nn.BCEWithLogitsLoss()

  def __call__(self, output_fixation, true_fixation):
    output_fixation = output_fixation
    return self.loss_function(self.downsample(output_fixation), self.downsample(true_fixation))

loss_function = BCELossWithDownsampling() if TRAINING_LOSS_CHOICE == LOSS_BCE else WeightedMSELoss()

alternative_loss_function = WeightedMSELoss() if TRAINING_LOSS_CHOICE == LOSS_BCE else BCELossWithDownsampling()

"""# Validation"""

@contextmanager
def evaluating(net):
  istrain = net.training
  try:
    net.eval()
    yield net
  finally:
    if istrain:
      net.train()

def validate(dataset=ds_validation, used_loss_function=loss_function):
  with evaluating(net):
    results = []

    with torch.no_grad():
      for sample in dataset:
        image, fixation = sample['image'].unsqueeze(0).to(device), sample['fixation'].unsqueeze(0).to(device)
        output = net(image)

        results.append(used_loss_function(output, fixation).item())

    results = np.array(results)
    average_loss = np.average(results)

    return average_loss, results

"""# Training"""

class TrainingStatusDisplayer:
  def __init__(self, start_time=None):
    self.start_time = start_time or time()
    self.last_display_time = -1
    self.batches_per_epoch = len(dl_train)

  def print(self, *args):
    args_formatted = []

    for arg in args:
      if isinstance(arg, float):
        arg = "{:.2f}".format(arg)
      args_formatted.append(arg)

    print(*args_formatted)

  def show(self, epoch_index, batch_index, training_losses, validation_losses, training_losses_epoch):
    if (time() - self.last_display_time) < TRAINING_STATUS_DISPLAY_INTERVAL: return

    self.last_display_time = time()
    clear_output(wait=True)

    global_progress = (epoch_index * self.batches_per_epoch + batch_index) / (TRAINING_EPOCHS * self.batches_per_epoch)
    ran_for = time() - self.start_time
    
    self.print('Global progress:', 100 * global_progress, '% done')
    self.print('  Ran for', ran_for, 'seconds')
    self.print('  Expected running time', ran_for/global_progress if global_progress > 0 else '?', 'seconds')
    self.print('  Expected remaining time', ran_for/global_progress - ran_for if global_progress > 0 else '?', 'seconds')
    self.print('Current epoch:', epoch_index + 1, 'of', TRAINING_EPOCHS)
    self.print('  Running batch', batch_index + 1, 'of', self.batches_per_epoch)
    self.print(' ', 100 * batch_index / self.batches_per_epoch, '% done')

    fig, ax = plt.subplots(1, 2, figsize=(17, 7))

    self.print('Latest training losses current epoch (by batch):\n', ', '.join(str(l) for l in training_losses_epoch[-5:]))
    self.print('Training losses:\n', ', '.join(str(l) for l in training_losses))

    real_validation_losses, alternative_validation_losses = zip(*validation_losses) if len(validation_losses) >= 1 else [tuple(), tuple()]
    self.print('Validation losses:\n', ', '.join(str(l) for l in real_validation_losses))
    self.print('Alternative validation losses:\n', ', '.join(str(l) for l in alternative_validation_losses))

    if not PRODUCTION_MODE:
      if len(training_losses_epoch) >= 2:
        ax[0].plot([l if l <= 1 else 1 for l in training_losses_epoch], label='Training loss current epoch (by batch)');
        ax[0].legend()

      if len(training_losses) >= 2:
        ax[1].plot(training_losses, label='Training loss');
        ax[1].plot(real_validation_losses, label='Validation loss')
        ax[1].plot(alternative_validation_losses, label='Alternative validation loss')
        ax[1].legend()

      plt.show()

def get_checkpoint_path(id):
  return CHECKPOINT_FILE_FOLDER + CHECKPOINT_FILE_PREFIX + str(id) + CHECKPOINT_FILE_POSTFIX

def get_newest_checkpoint_id():
  import re
  pattern = re.compile(CHECKPOINT_FILE_PREFIX + "(\d+)" + CHECKPOINT_FILE_POSTFIX)
  ids = []

  for name in os.listdir(CHECKPOINT_FILE_FOLDER):
    match = pattern.match(name)
    if match is not None: ids.append(int(match[1]))

  return max(ids) if len(ids) > 0 else None

def save_training_checkpoint(model, optimizer, epoch_index, training_losses, validation_losses):
  torch.save({
      'model_state_dict': model.state_dict(),
      'optimizer_state_dict': optimizer.state_dict(),
      'epoch_index': epoch_index,
      'training_losses': training_losses,
      'validation_losses': validation_losses,
  }, get_checkpoint_path(1 + (get_newest_checkpoint_id() or 0)))

def load_training_checkpoint(model=None, optimizer=None, id=None):
  if id is None:
    id = get_newest_checkpoint_id()

  if id is None:
    return 0, [], []

  checkpoint = torch.load(get_checkpoint_path(id))
  if model: model.load_state_dict(checkpoint['model_state_dict'])
  if optimizer: optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
  
  return checkpoint['epoch_index'], checkpoint['training_losses'], checkpoint['validation_losses']

def train():
  status_displayer = TrainingStatusDisplayer()
  optimizer = torch.optim.AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
  latest_validation_results = None

  epoch_index, training_losses, validation_losses = load_training_checkpoint(net, optimizer)
  net.train()

  while epoch_index < TRAINING_EPOCHS:
    training_losses_epoch = []

    for batch_index, batch in enumerate(dl_train):
      status_displayer.show(epoch_index, batch_index, training_losses, validation_losses, training_losses_epoch)

      image, fixation = batch['image'].to(device), batch['fixation'].to(device)

      output = net(image)

      loss = loss_function(output, fixation)
      training_losses_epoch.append(loss.item())

      loss.backward()

      optimizer.step()
      optimizer.zero_grad()
    
    training_losses.append(np.average(training_losses_epoch))
    validation_loss, latest_validation_results = validate()
    alternative_validation_loss, _ = validate(used_loss_function=alternative_loss_function)
    validation_losses.append((validation_loss, alternative_validation_loss))

    epoch_index += 1
    save_training_checkpoint(net, optimizer, epoch_index, training_losses, validation_losses)

  return np.array(training_losses), np.array(validation_losses), latest_validation_results

training_losses, validation_losses, validation_results = train()

epoch_index, training_losses, validation_losses = load_training_checkpoint()
TrainingStatusDisplayer().show(epoch_index, 0, training_losses, validation_losses, [])

"""# Analyzing results"""

validation_loss = None

if validation_results is None:
  validation_loss, validation_results = validate()
else: 
  validation_loss = validation_losses[-1]

validation_loss, validation_results

validation_indices_sorted = np.argsort(validation_results)
validation_indices_sorted

validation_results[validation_indices_sorted]

def display_validation_example(index, ds=ds_validation, ds_notransform=ds_validation_notransform):
  if PRODUCTION_MODE: return
  output = None

  with evaluating(net):
    with torch.no_grad():
      input = ds[index]['image'].unsqueeze(0).to(device)
      output = torch.sigmoid(net(input))
      output = output.cpu().numpy()[0][0]

  image, fixation = ds_notransform[index]['image']/255, ds_notransform[index]['fixation']/255

  fig, ax = plt.subplots(1, 3, figsize=(18, 6))
  ax[0].imshow(image, norm=None, vmin=0, vmax=1)
  ax[0].set_title('Input image')

  ax[1].imshow(fixation, norm=None, vmin=0, vmax=1)
  ax[1].set_title('True fixation')

  ax[2].imshow(output, norm=None, vmin=0, vmax=1)
  ax[2].set_title('Predicted fixation')

  print("True fixation flat, sorted:", np.sort(fixation.flatten()))
  print("Predicted fixation flat, sorted:", np.sort(output.flatten()))

"""## Positive examples"""

display_validation_example(validation_indices_sorted[0])

display_validation_example(validation_indices_sorted[1])

display_validation_example(validation_indices_sorted[2])

"""## Negative examples"""

display_validation_example(validation_indices_sorted[-1])

display_validation_example(validation_indices_sorted[-2])

display_validation_example(validation_indices_sorted[-3])

"""## Medium examples"""

display_validation_example(validation_indices_sorted[len(validation_indices_sorted)//2])

display_validation_example(validation_indices_sorted[1 + len(validation_indices_sorted)//2])

display_validation_example(validation_indices_sorted[-1 + len(validation_indices_sorted)//2])